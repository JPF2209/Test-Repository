Here are the top 5 regression risks that exist in Rhombus AI. The first is data ingestion failures such as file upload limits and format parsing which is the entry point for all pipelines where if this fails, it blocks all processing and user productivity. This usually occurs due to changes in supported file formats, size limits or cloud connector changes and the easiest way to mitigate includes testing the network and API, do data validation testing and UI testing. The second risk is transformation logic errors such as incorrect parameter handling in transformers where this shapes and cleans data where this failing leads to incorrect data in general impacting user trust. This usually occurs due to transformer updates and new features increasing this risk and again, to mitigate these risks, use data validation, UI and API testing. The third risk is AI Assisted Pipeline Generation where failures in AI usage can reduce automation benefits but the cause of these risks are usually AI model updates and context handling. Tests to mitigate this risk include API and UI testing. The fourth risk is pipeline execution and management such as running and caching where this needs to be reliable and perform well or it will lead to delays or incorrect results usually coming from changes in the execution engine, caching logic or parallel processing. Tests to mitigate this risk include UI, API and Data Validation. The last risk is data export and download functionality where the results fail to work losing trust and the results needing rework where the largest risks include updates to parts of the system dealing with this functionality where testing for UP, API and Data Validation mitigates this.

For automation prioritisation, what should be automated first are either critical data flows, UI flows for pipeline creation and run or data validation tests since they deliver high value whilst having stable interfaces and good signal quality reducing manual regression effort increasing confidence in core workflows. What shouldnâ€™t be prioritised include full AI assisted pipeline output assertions, highly visual or exploratory UI elements and rare edge cases in transformation parameters. This is because these are high cost, unstable and low ROI processes for automation making it inefficient and difficult to automate until interfaces stabilise themselves.

For each testing layer, UI end to end testing validates complete user workflows that include project creation, data upload and more where failures caught include UI regressions, broken flows and missing or misconfigured UI elements where this layer ensures that the end-user experience is positive. For the API or network level testing, this tests backend endpoints for data ingestion, AI chat, transformation execution and more caching failures such as performance issues, contract breaks and authentication errors where this ensures that backend processes are robust and operate correctly. Then for data validation and correctness testing, this verifies the correctness, completeness and quality of data at each pipeline stage checking for silent data corruption, transformation logic errors and missing or malformed data ensuring the output is correct and has a high-level of integrity.


